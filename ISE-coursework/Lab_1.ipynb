{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3ybN7wBDJd_l",
        "outputId": "f53823af-0ce7-480f-bef6-4be4fcbaa6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ISE-solution'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (209/209), done.\u001b[K\n",
            "remote: Total 210 (delta 88), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (210/210), 14.72 MiB | 4.76 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "lab1文件夹克隆成功，文件夹内容如下：\n",
            "lab1/\n",
            "    br_classification.py\n",
            "    README.md\n",
            "    datasets/\n",
            "        caffe.csv\n",
            "        tensorflow.csv\n",
            "        keras.csv\n",
            "        incubator-mxnet.csv\n",
            "        pytorch.csv\n",
            "Cloning into 'ISE-solution'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (209/209), done.\u001b[K\n",
            "remote: Total 210 (delta 88), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (210/210), 14.72 MiB | 4.73 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "lab1文件夹克隆成功，文件夹内容如下：\n",
            "lab1/\n",
            "    br_classification.py\n",
            "    README.md\n",
            "    datasets/\n",
            "        caffe.csv\n",
            "        tensorflow.csv\n",
            "        keras.csv\n",
            "        incubator-mxnet.csv\n",
            "        pytorch.csv\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ISE-solution\n",
        "# 克隆整个仓库到Colab环境\n",
        "!git clone https://github.com/ideas-labo/ISE-solution\n",
        "\n",
        "# 查看克隆下来的lab2文件夹结构，以确认克隆成功\n",
        "import os\n",
        "lab1_path = '/content/ISE-solution/lab1'\n",
        "if os.path.exists(lab1_path):\n",
        "    print(\"lab1文件夹克隆成功，文件夹内容如下：\")\n",
        "    for root, dirs, files in os.walk(lab1_path):\n",
        "        level = root.replace(lab1_path, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        sub_indent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print('{}{}'.format(sub_indent, f))\n",
        "else:\n",
        "    print(\"lab1文件夹克隆失败，请检查网络或仓库地址。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WROdlLtRaBjK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "########## 1. Import required libraries ##########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Text and feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Evaluation and tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc)\n",
        "\n",
        "# Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Text cleaning & stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "########## 2. Define text preprocessing methods ##########\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Stopwords\n",
        "NLTK_stop_words_list = stopwords.words('english')\n",
        "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
        "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords from the text.\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Clean text by removing non-alphanumeric characters,\n",
        "    and convert it to lowercase.\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "########## 3. Download & read data ##########\n",
        "import os\n",
        "import subprocess\n",
        "# Choose the project (options: 'pytorch', 'tensorflow', 'keras', 'incubator-mxnet', 'caffe')\n",
        "project = 'caffe'\n",
        "path = '/content/ISE-solution/lab1/datasets/caffe.csv'\n",
        "\n",
        "pd_all = pd.read_csv(path)\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "\n",
        "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
        "pd_tplusb = pd_all.rename(columns={\n",
        "    \"Unnamed: 0\": \"id\",\n",
        "    \"class\": \"sentiment\",\n",
        "    \"Title+Body\": \"text\"\n",
        "})\n",
        "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
        "\n",
        "########## 4. Configure parameters & Start training ##########\n",
        "\n",
        "# ========== Key Configurations ==========\n",
        "\n",
        "# 1) Data file to read\n",
        "datafile = 'Title+Body.csv'\n",
        "\n",
        "# 2) Number of repeated experiments\n",
        "REPEAT = 30\n",
        "\n",
        "# 3) Output CSV file name\n",
        "out_csv_name = f'../{project}_NB.csv'\n",
        "\n",
        "# ========== Read and clean data ==========\n",
        "data = pd.read_csv(datafile).fillna('')\n",
        "text_col = 'text'\n",
        "\n",
        "# Keep a copy for referencing original data if needed\n",
        "original_data = data.copy()\n",
        "\n",
        "# Text cleaning\n",
        "data[text_col] = data[text_col].apply(remove_html)\n",
        "data[text_col] = data[text_col].apply(remove_emoji)\n",
        "data[text_col] = data[text_col].apply(remove_stopwords)\n",
        "data[text_col] = data[text_col].apply(clean_str)\n",
        "\n",
        "# ========== Hyperparameter grid ==========\n",
        "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
        "params = {\n",
        "    'var_smoothing': np.logspace(-12, 0, 13)\n",
        "}\n",
        "\n",
        "# Lists to store metrics across repeated runs\n",
        "accuracies  = []\n",
        "precisions  = []\n",
        "recalls     = []\n",
        "f1_scores   = []\n",
        "auc_values  = []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    # --- 4.1 Split into train/test ---\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(\n",
        "        indices, test_size=0.2, random_state=repeated_time\n",
        "    )\n",
        "\n",
        "    train_text = data[text_col].iloc[train_index]\n",
        "    test_text = data[text_col].iloc[test_index]\n",
        "\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test  = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    # --- 4.2 TF-IDF vectorization ---\n",
        "    tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=1000  # Adjust as needed\n",
        "    )\n",
        "    X_train = tfidf.fit_transform(train_text)\n",
        "    X_test = tfidf.transform(test_text)\n",
        "\n",
        "    # Convert sparse matrices to dense arrays\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "\n",
        "    # --- 4.3 Naive Bayes model & GridSearch ---\n",
        "    clf = GaussianNB()\n",
        "    grid = GridSearchCV(\n",
        "        clf,\n",
        "        params,\n",
        "        cv=5,              # 5-fold CV (can be changed)\n",
        "        scoring='roc_auc'  # Using roc_auc as the metric for selection\n",
        "    )\n",
        "    grid.fit(X_train_dense, y_train)  # Use dense array here\n",
        "\n",
        "    # Retrieve the best model\n",
        "    best_clf = grid.best_estimator_\n",
        "    best_clf.fit(X_train_dense, y_train)  # Also use dense array here\n",
        "\n",
        "    # --- 4.4 Make predictions & evaluate ---\n",
        "    y_pred = best_clf.predict(X_test_dense)  # Use dense array for prediction\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    # Precision (macro)\n",
        "    prec = precision_score(y_test, y_pred, average='macro')\n",
        "    precisions.append(prec)\n",
        "\n",
        "    # Recall (macro)\n",
        "    rec = recall_score(y_test, y_pred, average='macro')\n",
        "    recalls.append(rec)\n",
        "\n",
        "    # F1 Score (macro)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    auc_val = auc(fpr, tpr)\n",
        "    auc_values.append(auc_val)\n",
        "\n",
        "# --- 4.5 Aggregate results ---\n",
        "final_accuracy  = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall    = np.mean(recalls)\n",
        "final_f1        = np.mean(f1_scores)\n",
        "final_auc       = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Naive Bayes + TF-IDF Results ===\")\n",
        "print(f\"Number of repeats:     {REPEAT}\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "# Save final results to CSV (append mode)\n",
        "try:\n",
        "    existing_data = pd.read_csv(out_csv_name, nrows=1)\n",
        "    header_needed = False\n",
        "except:\n",
        "    header_needed = True\n",
        "\n",
        "df_log = pd.DataFrame(\n",
        "    {\n",
        "        'repeated_times': [REPEAT],\n",
        "        'Accuracy': [final_accuracy],\n",
        "        'Precision': [final_precision],\n",
        "        'Recall': [final_recall],\n",
        "        'F1': [final_f1],\n",
        "        'AUC': [final_auc],\n",
        "        'CV_list(AUC)': [str(auc_values)]\n",
        "    }\n",
        ")\n",
        "\n",
        "df_log.to_csv(out_csv_name, mode='a', header=header_needed, index=False)\n",
        "\n",
        "print(f\"\\nResults have been saved to: {out_csv_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XYYtCyzGac4Z"
      },
      "outputs": [],
      "source": [
        "pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3hAugDpxc7p2"
      },
      "outputs": [],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rCBSHuXpdG_D"
      },
      "outputs": [],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EZwaoP9igg0R"
      },
      "outputs": [],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C7paVWk51eB7"
      },
      "outputs": [],
      "source": [
        "pip install numpy gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E--2aki6pew2",
        "outputId": "d3159531-dbef-4222-ea2c-9152f22e0a43"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized results saved to: /content/ISE-solution/lab1/results/caffe_improved.csv\n",
            "       repeated_times  Accuracy  Precision    Recall        F1       AUC\n",
            "count             5.0  5.000000   5.000000  5.000000  5.000000  5.000000\n",
            "mean             30.0  0.894598   0.777645  0.646838  0.669200  0.862350\n",
            "std               0.0  0.051091   0.136582  0.072807  0.092827  0.057900\n",
            "min              30.0  0.810345   0.604449  0.573718  0.597222  0.778846\n",
            "25%              30.0  0.896552   0.705357  0.580342  0.609690  0.855769\n",
            "50%              30.0  0.904023   0.796970  0.657051  0.620915  0.855983\n",
            "75%              30.0  0.913793   0.808724  0.673077  0.698858  0.881410\n",
            "max              30.0  0.948276   0.972727  0.750000  0.819315  0.939744\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import defaultdict\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text cleaning function definitions\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "# Configuration parameters\n",
        "input_path = '/content/ISE-solution/lab1/datasets/caffe.csv'\n",
        "output_dir = '/content/ISE-solution/lab1/results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file = os.path.join(output_dir, 'caffe_improved.csv')\n",
        "# Advanced text preprocessing\n",
        "def advanced_cleaning(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = remove_html(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in final_stop_words_list]\n",
        "    return ' '.join(words)\n",
        "# Load data\n",
        "df = pd.read_csv(input_path)\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df['combined_text'] = df['Title'] + '. ' + df['Body'].fillna('')\n",
        "# Data preprocessing\n",
        "final_stop_words_list = stopwords.words('english') + ['...', 'caffe', 'model', 'layer']\n",
        "df['clean_text'] = df['combined_text'].apply(advanced_cleaning)\n",
        "# Split dataset\n",
        "X = df['clean_text']\n",
        "y = df['class']\n",
        "# Define models and parameter grids\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(max_iter=1000),\n",
        "        'params': [\n",
        "            {\n",
        "                'penalty': ['l2'],\n",
        "                'C': [0.1, 1, 10],\n",
        "                'class_weight': ['balanced', None]\n",
        "            },\n",
        "            {\n",
        "                'penalty': [None],\n",
        "                'class_weight': ['balanced', None]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(probability=True),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'learning_rate': [0.1, 0.01],\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5]\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 1, 10],\n",
        "            'fit_prior': [True, False]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "# Results container\n",
        "results = defaultdict(list)\n",
        "# Multiple experiment configuration\n",
        "REPEAT = 30\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for model_name, config in models.items():\n",
        "    # Initialize temporary metric lists for each model\n",
        "    acc_scores = []\n",
        "    prec_scores = []\n",
        "    rec_scores = []\n",
        "    f1_scores = []\n",
        "    auc_scores = []\n",
        "    cv_scores = []\n",
        "    for _ in range(REPEAT):\n",
        "        # Data splitting\n",
        "        train_idx, test_idx = next(kf.split(X))\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        # Feature engineering\n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=5000,\n",
        "            stop_words=final_stop_words_list\n",
        "        )\n",
        "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf.transform(X_test)\n",
        "        # Handle class imbalance\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train_tfidf, y_train)\n",
        "        # Grid search\n",
        "        grid = GridSearchCV(\n",
        "            config['model'],\n",
        "            config['params'],\n",
        "            cv=3,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid.fit(X_train_res, y_train_res)\n",
        "        # Best model prediction\n",
        "        best_clf = grid.best_estimator_\n",
        "        y_pred = best_clf.predict(X_test_tfidf)\n",
        "        y_proba = best_clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average='macro')\n",
        "        rec = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "        # Append metrics of each experiment to temporary lists\n",
        "        acc_scores.append(acc)\n",
        "        prec_scores.append(prec)\n",
        "        rec_scores.append(rec)\n",
        "        f1_scores.append(f1)\n",
        "        auc_scores.append(auc)\n",
        "        cv_scores.extend(grid.cv_results_['mean_test_score'])\n",
        "    # Calculate the mean of each metric for each model\n",
        "    results['Model'].append(model_name)\n",
        "    results['repeated_times'].append(REPEAT)\n",
        "    results['Accuracy'].append(np.mean(acc_scores))\n",
        "    results['Precision'].append(np.mean(prec_scores))\n",
        "    results['Recall'].append(np.mean(rec_scores))\n",
        "    results['F1'].append(np.mean(f1_scores))\n",
        "    results['AUC'].append(np.mean(auc_scores))\n",
        "    results['CV_list(AUC)'].append(str(np.mean(cv_scores)))\n",
        "# Save results\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(output_file, index=False)\n",
        "print(f\"Optimized results saved to: {output_file}\")\n",
        "print(final_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOLE8s5-hH6Z",
        "outputId": "853bf50f-82bb-4e10-e3d9-28b17c11da51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Naive Bayes + TF-IDF Results ===\n",
            "Number of repeats:     30\n",
            "Average Accuracy:      0.5971\n",
            "Average Precision:     0.6065\n",
            "Average Recall:        0.7444\n",
            "Average F1 score:      0.5345\n",
            "Average AUC:           0.7444\n",
            "\n",
            "Results have been saved to: ../incubator-mxnet_NB.csv\n"
          ]
        }
      ],
      "source": [
        "########## 1. Import required libraries ##########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Text and feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Evaluation and tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc)\n",
        "\n",
        "# Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Text cleaning & stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "########## 2. Define text preprocessing methods ##########\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Stopwords\n",
        "NLTK_stop_words_list = stopwords.words('english')\n",
        "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
        "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords from the text.\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Clean text by removing non-alphanumeric characters,\n",
        "    and convert it to lowercase.\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "########## 3. Download & read data ##########\n",
        "import os\n",
        "import subprocess\n",
        "# Choose the project (options: 'pytorch', 'tensorflow', 'keras', 'incubator-mxnet', 'caffe')\n",
        "project = 'incubator-mxnet'\n",
        "path = '/content/ISE-solution/lab1/datasets/incubator-mxnet.csv'\n",
        "\n",
        "pd_all = pd.read_csv(path)\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "\n",
        "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
        "pd_tplusb = pd_all.rename(columns={\n",
        "    \"Unnamed: 0\": \"id\",\n",
        "    \"class\": \"sentiment\",\n",
        "    \"Title+Body\": \"text\"\n",
        "})\n",
        "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
        "\n",
        "########## 4. Configure parameters & Start training ##########\n",
        "\n",
        "# ========== Key Configurations ==========\n",
        "\n",
        "# 1) Data file to read\n",
        "datafile = 'Title+Body.csv'\n",
        "\n",
        "# 2) Number of repeated experiments\n",
        "REPEAT = 30\n",
        "\n",
        "# 3) Output CSV file name\n",
        "out_csv_name = f'../{project}_NB.csv'\n",
        "\n",
        "# ========== Read and clean data ==========\n",
        "data = pd.read_csv(datafile).fillna('')\n",
        "text_col = 'text'\n",
        "\n",
        "# Keep a copy for referencing original data if needed\n",
        "original_data = data.copy()\n",
        "\n",
        "# Text cleaning\n",
        "data[text_col] = data[text_col].apply(remove_html)\n",
        "data[text_col] = data[text_col].apply(remove_emoji)\n",
        "data[text_col] = data[text_col].apply(remove_stopwords)\n",
        "data[text_col] = data[text_col].apply(clean_str)\n",
        "\n",
        "# ========== Hyperparameter grid ==========\n",
        "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
        "params = {\n",
        "    'var_smoothing': np.logspace(-12, 0, 13)\n",
        "}\n",
        "\n",
        "# Lists to store metrics across repeated runs\n",
        "accuracies  = []\n",
        "precisions  = []\n",
        "recalls     = []\n",
        "f1_scores   = []\n",
        "auc_values  = []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    # --- 4.1 Split into train/test ---\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(\n",
        "        indices, test_size=0.2, random_state=repeated_time\n",
        "    )\n",
        "\n",
        "    train_text = data[text_col].iloc[train_index]\n",
        "    test_text = data[text_col].iloc[test_index]\n",
        "\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test  = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    # --- 4.2 TF-IDF vectorization ---\n",
        "    tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=1000  # Adjust as needed\n",
        "    )\n",
        "    X_train = tfidf.fit_transform(train_text)\n",
        "    X_test = tfidf.transform(test_text)\n",
        "\n",
        "    # Convert sparse matrices to dense arrays\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "\n",
        "    # --- 4.3 Naive Bayes model & GridSearch ---\n",
        "    clf = GaussianNB()\n",
        "    grid = GridSearchCV(\n",
        "        clf,\n",
        "        params,\n",
        "        cv=5,              # 5-fold CV (can be changed)\n",
        "        scoring='roc_auc'  # Using roc_auc as the metric for selection\n",
        "    )\n",
        "    grid.fit(X_train_dense, y_train)  # Use dense array here\n",
        "\n",
        "    # Retrieve the best model\n",
        "    best_clf = grid.best_estimator_\n",
        "    best_clf.fit(X_train_dense, y_train)  # Also use dense array here\n",
        "\n",
        "    # --- 4.4 Make predictions & evaluate ---\n",
        "    y_pred = best_clf.predict(X_test_dense)  # Use dense array for prediction\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    # Precision (macro)\n",
        "    prec = precision_score(y_test, y_pred, average='macro')\n",
        "    precisions.append(prec)\n",
        "\n",
        "    # Recall (macro)\n",
        "    rec = recall_score(y_test, y_pred, average='macro')\n",
        "    recalls.append(rec)\n",
        "\n",
        "    # F1 Score (macro)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    auc_val = auc(fpr, tpr)\n",
        "    auc_values.append(auc_val)\n",
        "\n",
        "# --- 4.5 Aggregate results ---\n",
        "final_accuracy  = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall    = np.mean(recalls)\n",
        "final_f1        = np.mean(f1_scores)\n",
        "final_auc       = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Naive Bayes + TF-IDF Results ===\")\n",
        "print(f\"Number of repeats:     {REPEAT}\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "# Save final results to CSV (append mode)\n",
        "try:\n",
        "    existing_data = pd.read_csv(out_csv_name, nrows=1)\n",
        "    header_needed = False\n",
        "except:\n",
        "    header_needed = True\n",
        "\n",
        "df_log = pd.DataFrame(\n",
        "    {\n",
        "        'repeated_times': [REPEAT],\n",
        "        'Accuracy': [final_accuracy],\n",
        "        'Precision': [final_precision],\n",
        "        'Recall': [final_recall],\n",
        "        'F1': [final_f1],\n",
        "        'AUC': [final_auc],\n",
        "        'CV_list(AUC)': [str(auc_values)]\n",
        "    }\n",
        ")\n",
        "\n",
        "df_log.to_csv(out_csv_name, mode='a', header=header_needed, index=False)\n",
        "\n",
        "print(f\"\\nResults have been saved to: {out_csv_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0zG58NDiA30",
        "outputId": "d9cd92ae-4815-4484-fcd2-611d25742d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized results saved to: /content/ISE-solution/lab1/results/incubator-mxnet_improved.csv\n",
            "       repeated_times  Accuracy  Precision    Recall        F1       AUC\n",
            "count             5.0  5.000000   5.000000  5.000000  5.000000  5.000000\n",
            "mean             30.0  0.871282   0.709366  0.591722  0.604785  0.821468\n",
            "std               0.0  0.028403   0.115512  0.068505  0.083451  0.113870\n",
            "min              30.0  0.834615   0.591682  0.551648  0.559944  0.661426\n",
            "25%              30.0  0.855769   0.610544  0.554945  0.562105  0.751198\n",
            "50%              30.0  0.875000   0.684479  0.559158  0.565581  0.854720\n",
            "75%              30.0  0.880449   0.829162  0.580220  0.583128  0.900099\n",
            "max              30.0  0.910577   0.830964  0.712637  0.753169  0.939899\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Text cleaning function definitions\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "# Configuration parameters\n",
        "input_path = '/content/ISE-solution/lab1/datasets/incubator-mxnet.csv'\n",
        "output_dir = '/content/ISE-solution/lab1/results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file = os.path.join(output_dir, 'incubator-mxnet_improved.csv')\n",
        "\n",
        "\n",
        "# Advanced text preprocessing\n",
        "def advanced_cleaning(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = remove_html(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in final_stop_words_list]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input_path)\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df['combined_text'] = df['Title'] + '. ' + df['Body'].fillna('')\n",
        "\n",
        "# Data preprocessing\n",
        "final_stop_words_list = stopwords.words('english') + ['...', 'incubator-mxnet', 'model', 'layer', 'incubator', 'mxnet']\n",
        "df['clean_text'] = df['combined_text'].apply(advanced_cleaning)\n",
        "\n",
        "# Split dataset\n",
        "X = df['clean_text']\n",
        "y = df['class']\n",
        "\n",
        "# Define models and parameter grids\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(max_iter=5000, solver='liblinear'),\n",
        "        'params': [\n",
        "            {\n",
        "                'penalty': ['l2'],\n",
        "                'C': [0.1, 1, 10],\n",
        "                'class_weight': ['balanced', None]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(probability=True),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'learning_rate': [0.1, 0.01],\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5]\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 1, 10],\n",
        "            'fit_prior': [True, False]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Results container\n",
        "results = defaultdict(list)\n",
        "\n",
        "# Multiple experiment configuration\n",
        "REPEAT = 30\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for model_name, config in models.items():\n",
        "    # Initialize temporary metric lists for each model\n",
        "    acc_scores = []\n",
        "    prec_scores = []\n",
        "    rec_scores = []\n",
        "    f1_scores = []\n",
        "    auc_scores = []\n",
        "    cv_scores = []\n",
        "    for _ in range(REPEAT):\n",
        "        # Data splitting\n",
        "        train_idx, test_idx = next(kf.split(X))\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Feature engineering\n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=5000,\n",
        "            stop_words=final_stop_words_list\n",
        "        )\n",
        "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "        scaler = StandardScaler(with_mean=False)\n",
        "        X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
        "        X_test_tfidf = scaler.transform(X_test_tfidf)\n",
        "\n",
        "        # Handle class imbalance\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "        # Randomized search\n",
        "        n_iter = min(10, len(config['params']))\n",
        "        grid = RandomizedSearchCV(\n",
        "            config['model'],\n",
        "            config['params'],\n",
        "            cv=3,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            n_iter=n_iter\n",
        "        )\n",
        "        grid.fit(X_train_res, y_train_res)\n",
        "\n",
        "        # Best model prediction\n",
        "        best_clf = grid.best_estimator_\n",
        "        y_pred = best_clf.predict(X_test_tfidf)\n",
        "        y_proba = best_clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "        rec = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "        # Append metrics of each experiment to temporary lists\n",
        "        acc_scores.append(acc)\n",
        "        prec_scores.append(prec)\n",
        "        rec_scores.append(rec)\n",
        "        f1_scores.append(f1)\n",
        "        auc_scores.append(auc)\n",
        "        cv_scores.extend(grid.cv_results_['mean_test_score'])\n",
        "\n",
        "    # Calculate the mean of each metric for each model\n",
        "    results['Model'].append(model_name)\n",
        "    results['repeated_times'].append(REPEAT)\n",
        "    results['Accuracy'].append(np.mean(acc_scores))\n",
        "    results['Precision'].append(np.mean(prec_scores))\n",
        "    results['Recall'].append(np.mean(rec_scores))\n",
        "    results['F1'].append(np.mean(f1_scores))\n",
        "    results['AUC'].append(np.mean(auc_scores))\n",
        "    results['CV_list(AUC)'].append(str(np.mean(cv_scores)))\n",
        "\n",
        "# Save results\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(output_file, index=False)\n",
        "print(f\"Optimized results saved to: {output_file}\")\n",
        "print(final_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5pAmy07iwA0",
        "outputId": "2af25499-46b3-4ac1-95bb-1232d67e3f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Naive Bayes + TF-IDF Results ===\n",
            "Number of repeats:     30\n",
            "Average Accuracy:      0.5637\n",
            "Average Precision:     0.6307\n",
            "Average Recall:        0.6937\n",
            "Average F1 score:      0.5450\n",
            "Average AUC:           0.6937\n",
            "\n",
            "Results have been saved to: ../keras_NB.csv\n"
          ]
        }
      ],
      "source": [
        "########## 1. Import required libraries ##########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Text and feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Evaluation and tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc)\n",
        "\n",
        "# Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Text cleaning & stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "########## 2. Define text preprocessing methods ##########\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Stopwords\n",
        "NLTK_stop_words_list = stopwords.words('english')\n",
        "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
        "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords from the text.\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Clean text by removing non-alphanumeric characters,\n",
        "    and convert it to lowercase.\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "########## 3. Download & read data ##########\n",
        "import os\n",
        "import subprocess\n",
        "# Choose the project (options: 'pytorch', 'tensorflow', 'keras', 'incubator-mxnet', 'caffe')\n",
        "project = 'keras'\n",
        "path = '/content/ISE-solution/lab1/datasets/keras.csv'\n",
        "\n",
        "pd_all = pd.read_csv(path)\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "\n",
        "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
        "pd_tplusb = pd_all.rename(columns={\n",
        "    \"Unnamed: 0\": \"id\",\n",
        "    \"class\": \"sentiment\",\n",
        "    \"Title+Body\": \"text\"\n",
        "})\n",
        "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
        "\n",
        "########## 4. Configure parameters & Start training ##########\n",
        "\n",
        "# ========== Key Configurations ==========\n",
        "\n",
        "# 1) Data file to read\n",
        "datafile = 'Title+Body.csv'\n",
        "\n",
        "# 2) Number of repeated experiments\n",
        "REPEAT = 30\n",
        "\n",
        "# 3) Output CSV file name\n",
        "out_csv_name = f'../{project}_NB.csv'\n",
        "\n",
        "# ========== Read and clean data ==========\n",
        "data = pd.read_csv(datafile).fillna('')\n",
        "text_col = 'text'\n",
        "\n",
        "# Keep a copy for referencing original data if needed\n",
        "original_data = data.copy()\n",
        "\n",
        "# Text cleaning\n",
        "data[text_col] = data[text_col].apply(remove_html)\n",
        "data[text_col] = data[text_col].apply(remove_emoji)\n",
        "data[text_col] = data[text_col].apply(remove_stopwords)\n",
        "data[text_col] = data[text_col].apply(clean_str)\n",
        "\n",
        "# ========== Hyperparameter grid ==========\n",
        "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
        "params = {\n",
        "    'var_smoothing': np.logspace(-12, 0, 13)\n",
        "}\n",
        "\n",
        "# Lists to store metrics across repeated runs\n",
        "accuracies  = []\n",
        "precisions  = []\n",
        "recalls     = []\n",
        "f1_scores   = []\n",
        "auc_values  = []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    # --- 4.1 Split into train/test ---\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(\n",
        "        indices, test_size=0.2, random_state=repeated_time\n",
        "    )\n",
        "\n",
        "    train_text = data[text_col].iloc[train_index]\n",
        "    test_text = data[text_col].iloc[test_index]\n",
        "\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test  = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    # --- 4.2 TF-IDF vectorization ---\n",
        "    tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=1000  # Adjust as needed\n",
        "    )\n",
        "    X_train = tfidf.fit_transform(train_text)\n",
        "    X_test = tfidf.transform(test_text)\n",
        "\n",
        "    # Convert sparse matrices to dense arrays\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "\n",
        "    # --- 4.3 Naive Bayes model & GridSearch ---\n",
        "    clf = GaussianNB()\n",
        "    grid = GridSearchCV(\n",
        "        clf,\n",
        "        params,\n",
        "        cv=5,              # 5-fold CV (can be changed)\n",
        "        scoring='roc_auc'  # Using roc_auc as the metric for selection\n",
        "    )\n",
        "    grid.fit(X_train_dense, y_train)  # Use dense array here\n",
        "\n",
        "    # Retrieve the best model\n",
        "    best_clf = grid.best_estimator_\n",
        "    best_clf.fit(X_train_dense, y_train)  # Also use dense array here\n",
        "\n",
        "    # --- 4.4 Make predictions & evaluate ---\n",
        "    y_pred = best_clf.predict(X_test_dense)  # Use dense array for prediction\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    # Precision (macro)\n",
        "    prec = precision_score(y_test, y_pred, average='macro')\n",
        "    precisions.append(prec)\n",
        "\n",
        "    # Recall (macro)\n",
        "    rec = recall_score(y_test, y_pred, average='macro')\n",
        "    recalls.append(rec)\n",
        "\n",
        "    # F1 Score (macro)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    auc_val = auc(fpr, tpr)\n",
        "    auc_values.append(auc_val)\n",
        "\n",
        "# --- 4.5 Aggregate results ---\n",
        "final_accuracy  = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall    = np.mean(recalls)\n",
        "final_f1        = np.mean(f1_scores)\n",
        "final_auc       = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Naive Bayes + TF-IDF Results ===\")\n",
        "print(f\"Number of repeats:     {REPEAT}\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "# Save final results to CSV (append mode)\n",
        "try:\n",
        "    existing_data = pd.read_csv(out_csv_name, nrows=1)\n",
        "    header_needed = False\n",
        "except:\n",
        "    header_needed = True\n",
        "\n",
        "df_log = pd.DataFrame(\n",
        "    {\n",
        "        'repeated_times': [REPEAT],\n",
        "        'Accuracy': [final_accuracy],\n",
        "        'Precision': [final_precision],\n",
        "        'Recall': [final_recall],\n",
        "        'F1': [final_f1],\n",
        "        'AUC': [final_auc],\n",
        "        'CV_list(AUC)': [str(auc_values)]\n",
        "    }\n",
        ")\n",
        "\n",
        "df_log.to_csv(out_csv_name, mode='a', header=header_needed, index=False)\n",
        "\n",
        "print(f\"\\nResults have been saved to: {out_csv_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-A9RTeRjiZK",
        "outputId": "b16826a8-1367-4d9c-becd-11210871c1ae"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized results saved to: /content/ISE-solution/lab1/results/keras_improved.csv\n",
            "       repeated_times  Accuracy  Precision    Recall        F1       AUC\n",
            "count             5.0  5.000000   5.000000  5.000000  5.000000  5.000000\n",
            "mean             30.0  0.864677   0.762286  0.740301  0.737490  0.880725\n",
            "std               0.0  0.050138   0.065727  0.057573  0.050655  0.061202\n",
            "min              30.0  0.776119   0.652961  0.685365  0.669951  0.773684\n",
            "25%              30.0  0.877363   0.770127  0.711842  0.714752  0.892865\n",
            "50%              30.0  0.880597   0.771473  0.723684  0.743295  0.903070\n",
            "75%              30.0  0.893781   0.786949  0.744737  0.751589  0.905906\n",
            "max              30.0  0.895522   0.829918  0.835877  0.807865  0.928099\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import defaultdict\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text cleaning function definitions\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "# Configuration parameters\n",
        "input_path = '/content/ISE-solution/lab1/datasets/keras.csv'\n",
        "output_dir = '/content/ISE-solution/lab1/results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file = os.path.join(output_dir, 'keras_improved.csv')\n",
        "# Advanced text preprocessing\n",
        "def advanced_cleaning(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = remove_html(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in final_stop_words_list]\n",
        "    return ' '.join(words)\n",
        "# Load data\n",
        "df = pd.read_csv(input_path)\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df['combined_text'] = df['Title'] + '. ' + df['Body'].fillna('')\n",
        "# Data preprocessing\n",
        "final_stop_words_list = stopwords.words('english') + ['...', 'keras', 'model', 'layer']\n",
        "df['clean_text'] = df['combined_text'].apply(advanced_cleaning)\n",
        "# Split dataset\n",
        "X = df['clean_text']\n",
        "y = df['class']\n",
        "# Define models and parameter grids\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(max_iter=1000),\n",
        "        'params': [\n",
        "            {\n",
        "                'penalty': ['l2'],\n",
        "                'C': [0.1, 1, 10],\n",
        "                'class_weight': ['balanced', None]\n",
        "            },\n",
        "            {\n",
        "                'penalty': [None],\n",
        "                'class_weight': ['balanced', None]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(probability=True),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'learning_rate': [0.1, 0.01],\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5]\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 1, 10],\n",
        "            'fit_prior': [True, False]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "# Results container\n",
        "results = defaultdict(list)\n",
        "# Multiple experiment configuration\n",
        "REPEAT = 30\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for model_name, config in models.items():\n",
        "    # Initialize temporary metric lists for each model\n",
        "    acc_scores = []\n",
        "    prec_scores = []\n",
        "    rec_scores = []\n",
        "    f1_scores = []\n",
        "    auc_scores = []\n",
        "    cv_scores = []\n",
        "    for _ in range(REPEAT):\n",
        "        # Data splitting\n",
        "        train_idx, test_idx = next(kf.split(X))\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        # Feature engineering\n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=5000,\n",
        "            stop_words=final_stop_words_list\n",
        "        )\n",
        "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf.transform(X_test)\n",
        "        # Handle class imbalance\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train_tfidf, y_train)\n",
        "        # Grid search\n",
        "        grid = GridSearchCV(\n",
        "            config['model'],\n",
        "            config['params'],\n",
        "            cv=3,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid.fit(X_train_res, y_train_res)\n",
        "        # Best model prediction\n",
        "        best_clf = grid.best_estimator_\n",
        "        y_pred = best_clf.predict(X_test_tfidf)\n",
        "        y_proba = best_clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average='macro')\n",
        "        rec = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "        # Append metrics of each experiment to temporary lists\n",
        "        acc_scores.append(acc)\n",
        "        prec_scores.append(prec)\n",
        "        rec_scores.append(rec)\n",
        "        f1_scores.append(f1)\n",
        "        auc_scores.append(auc)\n",
        "        cv_scores.extend(grid.cv_results_['mean_test_score'])\n",
        "    # Calculate the mean of each metric for each model\n",
        "    results['Model'].append(model_name)\n",
        "    results['repeated_times'].append(REPEAT)\n",
        "    results['Accuracy'].append(np.mean(acc_scores))\n",
        "    results['Precision'].append(np.mean(prec_scores))\n",
        "    results['Recall'].append(np.mean(rec_scores))\n",
        "    results['F1'].append(np.mean(f1_scores))\n",
        "    results['AUC'].append(np.mean(auc_scores))\n",
        "    results['CV_list(AUC)'].append(str(np.mean(cv_scores)))\n",
        "# Save results\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(output_file, index=False)\n",
        "print(f\"Optimized results saved to: {output_file}\")\n",
        "print(final_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_EPQE7lkJS3",
        "outputId": "8a47971d-69e1-48e9-8bdf-cd27086c90a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Naive Bayes + TF-IDF Results ===\n",
            "Number of repeats:     30\n",
            "Average Accuracy:      0.6402\n",
            "Average Precision:     0.6120\n",
            "Average Recall:        0.7542\n",
            "Average F1 score:      0.5651\n",
            "Average AUC:           0.7542\n",
            "\n",
            "Results have been saved to: ../pytorch_NB.csv\n"
          ]
        }
      ],
      "source": [
        "########## 1. Import required libraries ##########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Text and feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Evaluation and tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc)\n",
        "\n",
        "# Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Text cleaning & stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "########## 2. Define text preprocessing methods ##########\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Stopwords\n",
        "NLTK_stop_words_list = stopwords.words('english')\n",
        "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
        "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords from the text.\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Clean text by removing non-alphanumeric characters,\n",
        "    and convert it to lowercase.\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "########## 3. Download & read data ##########\n",
        "import os\n",
        "import subprocess\n",
        "# Choose the project (options: 'pytorch', 'tensorflow', 'keras', 'incubator-mxnet', 'caffe')\n",
        "project = 'pytorch'\n",
        "path = '/content/ISE-solution/lab1/datasets/pytorch.csv'\n",
        "\n",
        "pd_all = pd.read_csv(path)\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "\n",
        "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
        "pd_tplusb = pd_all.rename(columns={\n",
        "    \"Unnamed: 0\": \"id\",\n",
        "    \"class\": \"sentiment\",\n",
        "    \"Title+Body\": \"text\"\n",
        "})\n",
        "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
        "\n",
        "########## 4. Configure parameters & Start training ##########\n",
        "\n",
        "# ========== Key Configurations ==========\n",
        "\n",
        "# 1) Data file to read\n",
        "datafile = 'Title+Body.csv'\n",
        "\n",
        "# 2) Number of repeated experiments\n",
        "REPEAT = 30\n",
        "\n",
        "# 3) Output CSV file name\n",
        "out_csv_name = f'../{project}_NB.csv'\n",
        "\n",
        "# ========== Read and clean data ==========\n",
        "data = pd.read_csv(datafile).fillna('')\n",
        "text_col = 'text'\n",
        "\n",
        "# Keep a copy for referencing original data if needed\n",
        "original_data = data.copy()\n",
        "\n",
        "# Text cleaning\n",
        "data[text_col] = data[text_col].apply(remove_html)\n",
        "data[text_col] = data[text_col].apply(remove_emoji)\n",
        "data[text_col] = data[text_col].apply(remove_stopwords)\n",
        "data[text_col] = data[text_col].apply(clean_str)\n",
        "\n",
        "# ========== Hyperparameter grid ==========\n",
        "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
        "params = {\n",
        "    'var_smoothing': np.logspace(-12, 0, 13)\n",
        "}\n",
        "\n",
        "# Lists to store metrics across repeated runs\n",
        "accuracies  = []\n",
        "precisions  = []\n",
        "recalls     = []\n",
        "f1_scores   = []\n",
        "auc_values  = []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    # --- 4.1 Split into train/test ---\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(\n",
        "        indices, test_size=0.2, random_state=repeated_time\n",
        "    )\n",
        "\n",
        "    train_text = data[text_col].iloc[train_index]\n",
        "    test_text = data[text_col].iloc[test_index]\n",
        "\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test  = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    # --- 4.2 TF-IDF vectorization ---\n",
        "    tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=1000  # Adjust as needed\n",
        "    )\n",
        "    X_train = tfidf.fit_transform(train_text)\n",
        "    X_test = tfidf.transform(test_text)\n",
        "\n",
        "    # Convert sparse matrices to dense arrays\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "\n",
        "    # --- 4.3 Naive Bayes model & GridSearch ---\n",
        "    clf = GaussianNB()\n",
        "    grid = GridSearchCV(\n",
        "        clf,\n",
        "        params,\n",
        "        cv=5,              # 5-fold CV (can be changed)\n",
        "        scoring='roc_auc'  # Using roc_auc as the metric for selection\n",
        "    )\n",
        "    grid.fit(X_train_dense, y_train)  # Use dense array here\n",
        "\n",
        "    # Retrieve the best model\n",
        "    best_clf = grid.best_estimator_\n",
        "    best_clf.fit(X_train_dense, y_train)  # Also use dense array here\n",
        "\n",
        "    # --- 4.4 Make predictions & evaluate ---\n",
        "    y_pred = best_clf.predict(X_test_dense)  # Use dense array for prediction\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    # Precision (macro)\n",
        "    prec = precision_score(y_test, y_pred, average='macro')\n",
        "    precisions.append(prec)\n",
        "\n",
        "    # Recall (macro)\n",
        "    rec = recall_score(y_test, y_pred, average='macro')\n",
        "    recalls.append(rec)\n",
        "\n",
        "    # F1 Score (macro)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    auc_val = auc(fpr, tpr)\n",
        "    auc_values.append(auc_val)\n",
        "\n",
        "# --- 4.5 Aggregate results ---\n",
        "final_accuracy  = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall    = np.mean(recalls)\n",
        "final_f1        = np.mean(f1_scores)\n",
        "final_auc       = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Naive Bayes + TF-IDF Results ===\")\n",
        "print(f\"Number of repeats:     {REPEAT}\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "# Save final results to CSV (append mode)\n",
        "try:\n",
        "    existing_data = pd.read_csv(out_csv_name, nrows=1)\n",
        "    header_needed = False\n",
        "except:\n",
        "    header_needed = True\n",
        "\n",
        "df_log = pd.DataFrame(\n",
        "    {\n",
        "        'repeated_times': [REPEAT],\n",
        "        'Accuracy': [final_accuracy],\n",
        "        'Precision': [final_precision],\n",
        "        'Recall': [final_recall],\n",
        "        'F1': [final_f1],\n",
        "        'AUC': [final_auc],\n",
        "        'CV_list(AUC)': [str(auc_values)]\n",
        "    }\n",
        ")\n",
        "\n",
        "df_log.to_csv(out_csv_name, mode='a', header=header_needed, index=False)\n",
        "\n",
        "print(f\"\\nResults have been saved to: {out_csv_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ_BbirukXe_",
        "outputId": "80484315-ddc3-4a77-b38b-6bfbb5e5930f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized results saved to: /content/ISE-solution/lab1/results/pytorch_improved.csv\n",
            "       repeated_times  Accuracy  Precision    Recall        F1       AUC\n",
            "count             5.0  5.000000   5.000000  5.000000  5.000000  5.000000\n",
            "mean             30.0  0.889316   0.821026  0.740117  0.759274  0.890857\n",
            "std               0.0  0.018818   0.039055  0.086815  0.070314  0.027407\n",
            "min              30.0  0.862914   0.775867  0.610991  0.640859  0.860367\n",
            "25%              30.0  0.880795   0.787139  0.710001  0.748117  0.869488\n",
            "50%              30.0  0.888300   0.825319  0.759022  0.798282  0.887489\n",
            "75%              30.0  0.907285   0.851295  0.775919  0.802700  0.912073\n",
            "max              30.0  0.907285   0.865509  0.844652  0.806410  0.924869\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import defaultdict\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text cleaning function definitions\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "# Configuration parameters\n",
        "input_path = '/content/ISE-solution/lab1/datasets/pytorch.csv'\n",
        "output_dir = '/content/ISE-solution/lab1/results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file = os.path.join(output_dir, 'pytorch_improved.csv')\n",
        "# Advanced text preprocessing\n",
        "def advanced_cleaning(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = remove_html(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in final_stop_words_list]\n",
        "    return ' '.join(words)\n",
        "# Load data\n",
        "df = pd.read_csv(input_path)\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df['combined_text'] = df['Title'] + '. ' + df['Body'].fillna('')\n",
        "# Data preprocessing\n",
        "final_stop_words_list = stopwords.words('english') + ['...', 'pytorch', 'model', 'layer']\n",
        "df['clean_text'] = df['combined_text'].apply(advanced_cleaning)\n",
        "# Split dataset\n",
        "X = df['clean_text']\n",
        "y = df['class']\n",
        "# Define models and parameter grids\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(max_iter=1000),\n",
        "        'params': [\n",
        "            {\n",
        "                'penalty': ['l2'],\n",
        "                'C': [0.1, 1, 10],\n",
        "                'class_weight': ['balanced', None]\n",
        "            },\n",
        "            {\n",
        "                'penalty': [None],\n",
        "                'class_weight': ['balanced', None]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(probability=True),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'learning_rate': [0.1, 0.01],\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5]\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 1, 10],\n",
        "            'fit_prior': [True, False]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "# Results container\n",
        "results = defaultdict(list)\n",
        "# Multiple experiment configuration\n",
        "REPEAT = 30\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for model_name, config in models.items():\n",
        "    # Initialize temporary metric lists for each model\n",
        "    acc_scores = []\n",
        "    prec_scores = []\n",
        "    rec_scores = []\n",
        "    f1_scores = []\n",
        "    auc_scores = []\n",
        "    cv_scores = []\n",
        "    for _ in range(REPEAT):\n",
        "        # Data splitting\n",
        "        train_idx, test_idx = next(kf.split(X))\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        # Feature engineering\n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=5000,\n",
        "            stop_words=final_stop_words_list\n",
        "        )\n",
        "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf.transform(X_test)\n",
        "        # Handle class imbalance\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train_tfidf, y_train)\n",
        "        # Grid search\n",
        "        grid = GridSearchCV(\n",
        "            config['model'],\n",
        "            config['params'],\n",
        "            cv=3,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid.fit(X_train_res, y_train_res)\n",
        "        # Best model prediction\n",
        "        best_clf = grid.best_estimator_\n",
        "        y_pred = best_clf.predict(X_test_tfidf)\n",
        "        y_proba = best_clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average='macro')\n",
        "        rec = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "        # Append metrics of each experiment to temporary lists\n",
        "        acc_scores.append(acc)\n",
        "        prec_scores.append(prec)\n",
        "        rec_scores.append(rec)\n",
        "        f1_scores.append(f1)\n",
        "        auc_scores.append(auc)\n",
        "        cv_scores.extend(grid.cv_results_['mean_test_score'])\n",
        "    # Calculate the mean of each metric for each model\n",
        "    results['Model'].append(model_name)\n",
        "    results['repeated_times'].append(REPEAT)\n",
        "    results['Accuracy'].append(np.mean(acc_scores))\n",
        "    results['Precision'].append(np.mean(prec_scores))\n",
        "    results['Recall'].append(np.mean(rec_scores))\n",
        "    results['F1'].append(np.mean(f1_scores))\n",
        "    results['AUC'].append(np.mean(auc_scores))\n",
        "    results['CV_list(AUC)'].append(str(np.mean(cv_scores)))\n",
        "# Save results\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(output_file, index=False)\n",
        "print(f\"Optimized results saved to: {output_file}\")\n",
        "print(final_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66QiNeW3khF3",
        "outputId": "39e94c96-8afd-4c38-e785-1de3298ba5d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Naive Bayes + TF-IDF Results ===\n",
            "Number of repeats:     30\n",
            "Average Accuracy:      0.5551\n",
            "Average Precision:     0.6349\n",
            "Average Recall:        0.7106\n",
            "Average F1 score:      0.5362\n",
            "Average AUC:           0.7106\n",
            "\n",
            "Results have been saved to: ../tensorflow_NB.csv\n"
          ]
        }
      ],
      "source": [
        "########## 1. Import required libraries ##########\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Text and feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Evaluation and tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc)\n",
        "\n",
        "# Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Text cleaning & stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "########## 2. Define text preprocessing methods ##########\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Stopwords\n",
        "NLTK_stop_words_list = stopwords.words('english')\n",
        "custom_stop_words_list = ['...']  # You can customize this list as needed\n",
        "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords from the text.\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Clean text by removing non-alphanumeric characters,\n",
        "    and convert it to lowercase.\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "########## 3. Download & read data ##########\n",
        "import os\n",
        "import subprocess\n",
        "# Choose the project (options: 'pytorch', 'tensorflow', 'keras', 'incubator-mxnet', 'caffe')\n",
        "project = 'tensorflow'\n",
        "path = '/content/ISE-solution/lab1/datasets/tensorflow.csv'\n",
        "\n",
        "pd_all = pd.read_csv(path)\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "\n",
        "# Merge Title and Body into a single column; if Body is NaN, use Title only\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Keep only necessary columns: id, Number, sentiment, text (merged Title+Body)\n",
        "pd_tplusb = pd_all.rename(columns={\n",
        "    \"Unnamed: 0\": \"id\",\n",
        "    \"class\": \"sentiment\",\n",
        "    \"Title+Body\": \"text\"\n",
        "})\n",
        "pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\", \"Number\", \"sentiment\", \"text\"])\n",
        "\n",
        "########## 4. Configure parameters & Start training ##########\n",
        "\n",
        "# ========== Key Configurations ==========\n",
        "\n",
        "# 1) Data file to read\n",
        "datafile = 'Title+Body.csv'\n",
        "\n",
        "# 2) Number of repeated experiments\n",
        "REPEAT = 30\n",
        "\n",
        "# 3) Output CSV file name\n",
        "out_csv_name = f'../{project}_NB.csv'\n",
        "\n",
        "# ========== Read and clean data ==========\n",
        "data = pd.read_csv(datafile).fillna('')\n",
        "text_col = 'text'\n",
        "\n",
        "# Keep a copy for referencing original data if needed\n",
        "original_data = data.copy()\n",
        "\n",
        "# Text cleaning\n",
        "data[text_col] = data[text_col].apply(remove_html)\n",
        "data[text_col] = data[text_col].apply(remove_emoji)\n",
        "data[text_col] = data[text_col].apply(remove_stopwords)\n",
        "data[text_col] = data[text_col].apply(clean_str)\n",
        "\n",
        "# ========== Hyperparameter grid ==========\n",
        "# We use logspace for var_smoothing: [1e-12, 1e-11, ..., 1]\n",
        "params = {\n",
        "    'var_smoothing': np.logspace(-12, 0, 13)\n",
        "}\n",
        "\n",
        "# Lists to store metrics across repeated runs\n",
        "accuracies  = []\n",
        "precisions  = []\n",
        "recalls     = []\n",
        "f1_scores   = []\n",
        "auc_values  = []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    # --- 4.1 Split into train/test ---\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(\n",
        "        indices, test_size=0.2, random_state=repeated_time\n",
        "    )\n",
        "\n",
        "    train_text = data[text_col].iloc[train_index]\n",
        "    test_text = data[text_col].iloc[test_index]\n",
        "\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test  = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    # --- 4.2 TF-IDF vectorization ---\n",
        "    tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        max_features=1000  # Adjust as needed\n",
        "    )\n",
        "    X_train = tfidf.fit_transform(train_text)\n",
        "    X_test = tfidf.transform(test_text)\n",
        "\n",
        "    # Convert sparse matrices to dense arrays\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "\n",
        "    # --- 4.3 Naive Bayes model & GridSearch ---\n",
        "    clf = GaussianNB()\n",
        "    grid = GridSearchCV(\n",
        "        clf,\n",
        "        params,\n",
        "        cv=5,              # 5-fold CV (can be changed)\n",
        "        scoring='roc_auc'  # Using roc_auc as the metric for selection\n",
        "    )\n",
        "    grid.fit(X_train_dense, y_train)  # Use dense array here\n",
        "\n",
        "    # Retrieve the best model\n",
        "    best_clf = grid.best_estimator_\n",
        "    best_clf.fit(X_train_dense, y_train)  # Also use dense array here\n",
        "\n",
        "    # --- 4.4 Make predictions & evaluate ---\n",
        "    y_pred = best_clf.predict(X_test_dense)  # Use dense array for prediction\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    # Precision (macro)\n",
        "    prec = precision_score(y_test, y_pred, average='macro')\n",
        "    precisions.append(prec)\n",
        "\n",
        "    # Recall (macro)\n",
        "    rec = recall_score(y_test, y_pred, average='macro')\n",
        "    recalls.append(rec)\n",
        "\n",
        "    # F1 Score (macro)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    auc_val = auc(fpr, tpr)\n",
        "    auc_values.append(auc_val)\n",
        "\n",
        "# --- 4.5 Aggregate results ---\n",
        "final_accuracy  = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall    = np.mean(recalls)\n",
        "final_f1        = np.mean(f1_scores)\n",
        "final_auc       = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Naive Bayes + TF-IDF Results ===\")\n",
        "print(f\"Number of repeats:     {REPEAT}\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "# Save final results to CSV (append mode)\n",
        "try:\n",
        "    existing_data = pd.read_csv(out_csv_name, nrows=1)\n",
        "    header_needed = False\n",
        "except:\n",
        "    header_needed = True\n",
        "\n",
        "df_log = pd.DataFrame(\n",
        "    {\n",
        "        'repeated_times': [REPEAT],\n",
        "        'Accuracy': [final_accuracy],\n",
        "        'Precision': [final_precision],\n",
        "        'Recall': [final_recall],\n",
        "        'F1': [final_f1],\n",
        "        'AUC': [final_auc],\n",
        "        'CV_list(AUC)': [str(auc_values)]\n",
        "    }\n",
        ")\n",
        "\n",
        "df_log.to_csv(out_csv_name, mode='a', header=header_needed, index=False)\n",
        "\n",
        "print(f\"\\nResults have been saved to: {out_csv_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6KIAm1mlQBQ",
        "outputId": "76d946ed-6a73-442c-c954-463e955373e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Improved Naive Bayes + TF-IDF Results ===\n",
            "Number of repeats:     30\n",
            "Average Accuracy:      0.8364\n",
            "Average Precision:     0.6548\n",
            "Average Recall:        0.6924\n",
            "Average F1 score:      0.6656\n",
            "Average AUC:           0.6924\n",
            "\n",
            "Results saved to: /content/ISE-solution/lab1/results/tensorflow_improved.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Text and feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "# Text cleaning & stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Data balancing\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "########## 2. Define text preprocessing methods ##########\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_html(text):\n",
        "    \"\"\"Remove HTML tags using a regex.\"\"\"\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    \"\"\"Remove emojis using a regex pattern.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "NLTK_stop_words_list = stopwords.words('english')\n",
        "\n",
        "custom_stop_words_list = ['...', 'tensorflow', 'tensor', 'model', 'layer', 'function']\n",
        "final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove stopwords and technical terms from the text.\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Clean text by removing non-alphanumeric characters,\n",
        "    handling special cases, and converting to lowercase.\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\r\\n|\\n|\\r\", \" \", string)  # Handle newlines\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)       # Collapse multiple spaces\n",
        "    string = re.sub(r\"\\\\\", \"\", string)            # Remove backslashes\n",
        "    string = re.sub(r\"\\'\", \"\", string)            # Remove apostrophes\n",
        "    string = re.sub(r\"\\\"\", \"\", string)            # Remove quotes\n",
        "    return string.strip().lower()\n",
        "\n",
        "########## 3. Download & read data ##########\n",
        "project = 'tensorflow'\n",
        "input_path = '/content/ISE-solution/lab1/datasets/tensorflow.csv'\n",
        "output_dir = '/content/ISE-solution/lab1/results'\n",
        "\n",
        "# Create output directory if not exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "pd_all = pd.read_csv(input_path)\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "\n",
        "# Merge Title and Body\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "pd_tplusb = pd_all.rename(columns={\n",
        "    \"Unnamed: 0\": \"id\",\n",
        "    \"class\": \"sentiment\",\n",
        "    \"Title+Body\": \"text\"\n",
        "})\n",
        "\n",
        "########## 4. Configure parameters & Start training ##########\n",
        "\n",
        "# ========== Key Configurations ==========\n",
        "datafile = 'Title+Body.csv'\n",
        "REPEAT = 30\n",
        "out_csv_name = os.path.join(output_dir, f'{project}_improved.csv')\n",
        "\n",
        "# ========== Read and clean data ==========\n",
        "data = pd.read_csv(datafile).fillna('')\n",
        "text_col = 'text'\n",
        "\n",
        "# Text cleaning pipeline\n",
        "data[text_col] = data[text_col].apply(remove_html)\n",
        "data[text_col] = data[text_col].apply(remove_emoji)\n",
        "data[text_col] = data[text_col].apply(remove_stopwords)\n",
        "data[text_col] = data[text_col].apply(clean_str)\n",
        "data[text_col] = data[text_col].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))  # Stemming\n",
        "\n",
        "# ========== Hyperparameter grid ==========\n",
        "params = {\n",
        "    'alpha': [0.01, 0.1, 1, 10],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "# Lists to store metrics\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "auc_values = []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    # --- 4.1 Split into train/test ---\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(\n",
        "        indices, test_size=0.2, random_state=repeated_time\n",
        "    )\n",
        "\n",
        "    train_text = data[text_col].iloc[train_index]\n",
        "    test_text = data[text_col].iloc[test_index]\n",
        "\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    # --- 4.2 TF-IDF vectorization ---\n",
        "    tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 3),\n",
        "        max_features=2000,\n",
        "        min_df=2,\n",
        "        stop_words=final_stop_words_list\n",
        "    )\n",
        "    X_train = tfidf.fit_transform(train_text)\n",
        "    X_test = tfidf.transform(test_text)\n",
        "\n",
        "    # --- 4.3 Data balancing with SMOTE ---\n",
        "    sm = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = sm.fit_resample(X_train.toarray(), y_train)\n",
        "\n",
        "    # --- 4.4 Naive Bayes model & GridSearch ---\n",
        "    clf = MultinomialNB()\n",
        "    grid = GridSearchCV(\n",
        "        clf,\n",
        "        params,\n",
        "        cv=5,\n",
        "        scoring='f1_macro'\n",
        "    )\n",
        "    grid.fit(X_train_res, y_train_res)\n",
        "\n",
        "    best_clf = grid.best_estimator_\n",
        "    best_clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # --- 4.5 Make predictions & evaluate ---\n",
        "    y_pred = best_clf.predict(X_test.toarray())\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, average='macro')\n",
        "    rec = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
        "    auc_val = auc(fpr, tpr)\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    precisions.append(prec)\n",
        "    recalls.append(rec)\n",
        "    f1_scores.append(f1)\n",
        "    auc_values.append(auc_val)\n",
        "\n",
        "# --- 4.6 Aggregate results ---\n",
        "final_accuracy = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall = np.mean(recalls)\n",
        "final_f1 = np.mean(f1_scores)\n",
        "final_auc = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Improved Naive Bayes + TF-IDF Results ===\")\n",
        "print(f\"Number of repeats:     {REPEAT}\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "# --- 4.7 Save results to CSV ---\n",
        "df_log = pd.DataFrame({\n",
        "    'repeated_times': [REPEAT],\n",
        "    'Accuracy': [final_accuracy],\n",
        "    'Precision': [final_precision],\n",
        "    'Recall': [final_recall],\n",
        "    'F1': [final_f1],\n",
        "    'AUC': [final_auc],\n",
        "    'CV_list(AUC)': [str(auc_values)]\n",
        "})\n",
        "\n",
        "df_log.to_csv(out_csv_name, mode='a', header=not os.path.exists(out_csv_name), index=False)\n",
        "print(f\"\\nResults saved to: {out_csv_name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}